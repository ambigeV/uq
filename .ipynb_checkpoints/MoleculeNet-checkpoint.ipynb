{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84d544b8-99a8-4b49-85d1-5db061cfc775",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No normalization for SPS. Feature removed!\n",
      "No normalization for AvgIpc. Feature removed!\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
      "Skipped loading modules with transformers dependency. No module named 'transformers'\n",
      "cannot import name 'HuggingFaceModel' from 'deepchem.models.torch_models' (C:\\Users\\user\\anaconda3\\envs\\uq\\lib\\site-packages\\deepchem\\models\\torch_models\\__init__.py)\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (C:\\Users\\user\\anaconda3\\envs\\uq\\lib\\site-packages\\deepchem\\models\\torch_models\\__init__.py)\n",
      "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'lightning'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n"
     ]
    }
   ],
   "source": [
    "import deepchem as dc\n",
    "from deepchem.molnet import load_delaney, load_freesolv, load_qm7, load_lipo, load_tox21\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gpytorch\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# 1. Load data\n",
    "# tasks, datasets, transformers = load_freesolv(featurizer='ECFP')\n",
    "# tasks, datasets, transformers = load_tox21()\n",
    "# # tasks, datasets, transformers = load_delaney(featurizer='ECFP')\n",
    "\n",
    "\n",
    "# train_dc, valid_dc, test_dc = datasets\n",
    "\n",
    "# n_tasks = len(tasks)               # usually 1\n",
    "# n_features = train_dc.X.shape[1]      # ECFP length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c2040cec-ed4f-43fc-b9c4-61bef9620fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import load_dataset\n",
    "import math\n",
    "from nn_baseline import MyTorchRegressor, MyTorchRegressorMC, HeteroscedasticL2Loss, MCDropoutRegressorRefined, \\\n",
    "calculate_cutoff_error_data, mse_from_mean_prediction, evaluate_uq_metrics_from_interval, MyTorchClassifierHeteroscedastic, \\\n",
    "HeteroscedasticClassificationLoss, MCDropoutClassifierWrapper, roc_auc_score, evaluate_uq_metrics_classification, \\\n",
    "calculate_cutoff_classification_data, DenseDirichlet, EvidentialClassificationLoss, GradientClippingCallback\n",
    "from data_utils import auc_from_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e2a5955-cb03-4119-9aae-fabacf33267c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tasks: 12. Selecting indices: [0]\n",
      "Final Active Tasks: 1\n",
      "Train Y Shape: (6264, 1)\n"
     ]
    }
   ],
   "source": [
    "tasks, train_dc, valid_dc, test_dc, transformers = load_dataset(\n",
    "        dataset_name=\"tox21\",\n",
    "        split=\"random\",\n",
    "        task_indices=[0]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44bf8ca7-211f-4e12-9e70-29b51a693614",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_weights=True\n",
    "# mode=\"regression\"\n",
    "mode = \"classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08c909fc-ff75-4fd1-8884-db812ed475c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dc.y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c37b65e3-abab-431a-bb64-5f6ddb29642b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Deep Evidential Classification\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 106\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# --- 2. Train ---\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Deep Evidential Classification\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 106\u001b[0m \u001b[43mdc_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mgradientClip\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dc_model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# Convert numpy data to PyTorch tensors (DeepChem .X are typically numpy arrays)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\uq\\lib\\site-packages\\deepchem\\models\\torch_models\\torch_model.py:338\u001b[0m, in \u001b[0;36mTorchModel.fit\u001b[1;34m(self, dataset, nb_epoch, max_checkpoints_to_keep, checkpoint_interval, deterministic, restore, variables, loss, callbacks, all_losses)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    290\u001b[0m         dataset: Dataset,\n\u001b[0;32m    291\u001b[0m         nb_epoch: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    298\u001b[0m         callbacks: Union[Callable, List[Callable]] \u001b[38;5;241m=\u001b[39m [],\n\u001b[0;32m    299\u001b[0m         all_losses: Optional[List[\u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[0;32m    300\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Train this model on a dataset.\u001b[39;00m\n\u001b[0;32m    301\u001b[0m \n\u001b[0;32m    302\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m    The average loss over the most recent checkpoint interval\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 338\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_generator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnb_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_checkpoints_to_keep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_losses\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\uq\\lib\\site-packages\\deepchem\\models\\torch_models\\torch_model.py:440\u001b[0m, in \u001b[0;36mTorchModel.fit_generator\u001b[1;34m(self, generator, max_checkpoints_to_keep, checkpoint_interval, restore, variables, loss, callbacks, all_losses)\u001b[0m\n\u001b[0;32m    438\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m loss(outputs, labels, weights)\n\u001b[0;32m    439\u001b[0m batch_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m--> 440\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lr_schedule \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    442\u001b[0m     lr_schedule\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\uq\\lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m             )\n\u001b[1;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\uq\\lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\uq\\lib\\site-packages\\torch\\optim\\adam.py:223\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    211\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    213\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    214\u001b[0m         group,\n\u001b[0;32m    215\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    220\u001b[0m         state_steps,\n\u001b[0;32m    221\u001b[0m     )\n\u001b[1;32m--> 223\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\uq\\lib\\site-packages\\torch\\optim\\optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\uq\\lib\\site-packages\\torch\\optim\\adam.py:784\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    782\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 784\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\uq\\lib\\site-packages\\torch\\optim\\adam.py:611\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    609\u001b[0m     exp_avg_sq_sqrt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_foreach_sqrt(device_max_exp_avg_sqs)\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 611\u001b[0m     exp_avg_sq_sqrt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_sqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_exp_avg_sqs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    613\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)\n\u001b[0;32m    614\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_add_(exp_avg_sq_sqrt, eps)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_tasks = train_dc.y.shape[1]\n",
    "n_features = train_dc.X.shape[1]\n",
    "\n",
    "if mode == \"regression\":\n",
    "    # --- 1. Build Model & DeepChem wrapper (Using the structure from the second code block) ---\n",
    "    # The model outputs 4 parameters (gamma, v, alpha, beta) for each task.\n",
    "    # We assume 'DenseNormalGamma' is the model structure (mu, v, alpha, beta)\n",
    "    model = DenseNormalGamma(n_features, n_tasks)\n",
    "\n",
    "    # The custom evidential loss func(y_true, evidential_output)\n",
    "    loss = EvidentialRegressionLoss(coeff=reg_coeff)\n",
    "else:\n",
    "    model = DenseDirichlet(n_features, n_tasks * 2)\n",
    "\n",
    "    loss = EvidentialClassificationLoss()\n",
    "\n",
    "gradientClip = GradientClippingCallback()\n",
    "\n",
    "if mode == \"regression\":\n",
    "    dc_model = dc.models.TorchModel(\n",
    "        model=model,\n",
    "        loss=loss,\n",
    "        # The output types match the return structure of DenseNormalGamma:\n",
    "        output_types=['prediction', 'loss', 'var1', 'var2'],\n",
    "        batch_size=128,\n",
    "        learning_rate=1e-4,\n",
    "        # wandb=True,  # Set to True\n",
    "        # model_dir='deep-evidential-regression-run-{}'.format(run_id),\n",
    "        log_frequency=40,\n",
    "        mode='regression'\n",
    "    )\n",
    "\n",
    "    # --- 2. Train ---\n",
    "    print(f\"Training Deep Evidential Regression with lambda (reg_coeff) = {reg_coeff}\")\n",
    "    dc_model.fit(train_dc, nb_epoch=10, callbacks=[gradientClip])\n",
    "    device = next(dc_model.model.parameters()).device\n",
    "\n",
    "    # Convert numpy data to PyTorch tensors (DeepChem .X are typically numpy arrays)\n",
    "    valid_X_tensor = torch.from_numpy(valid_dc.X).float().to(device)\n",
    "    test_X_tensor = torch.from_numpy(test_dc.X).float().to(device)\n",
    "\n",
    "    # Get predictions for the validation set\n",
    "    with torch.no_grad():\n",
    "        mu_valid, params_valid, aleatoric_valid, epistemic_valid = dc_model.model(valid_X_tensor)\n",
    "\n",
    "    # Get predictions for the test set\n",
    "    with torch.no_grad():\n",
    "        mu_test, params_test, aleatoric_test, epistemic_test = dc_model.model(test_X_tensor)\n",
    "\n",
    "\n",
    "    # The total predictive variance Var[y] is the sum of aleatoric and epistemic variance.\n",
    "    # Var[y] = E[sigma^2] + Var[mu]\n",
    "    total_var_test = aleatoric_test.cpu().numpy() + epistemic_test.cpu().numpy()\n",
    "    std_test = np.sqrt(total_var_test)\n",
    "\n",
    "    # --- 4. Calculate MSE (using the deterministic mean prediction, gamma) ---\n",
    "    # Ensure prediction shape matches for MSE calculation\n",
    "    mu_test = mu_test.cpu().numpy()\n",
    "    if mu_test.ndim == 1:\n",
    "        mu_test = mu_test.reshape(-1, 1)\n",
    "\n",
    "    cutoff_error_df = calculate_cutoff_error_data(mu_test, total_var_test, test_dc.y, test_dc.w, use_weights=use_weights)\n",
    "\n",
    "    test_mse = mse_from_mean_prediction(mu_test, test_dc, use_weights=use_weights)\n",
    "\n",
    "    # --- 5. Calculate UQ Metrics ---\n",
    "    z = norm.ppf(1 - alpha / 2.0)\n",
    "\n",
    "    # Confidence interval: mean ± z * standard_deviation\n",
    "    lower = mu_test - z * std_test\n",
    "    upper = mu_test + z * std_test\n",
    "\n",
    "    # Ensure std_test is broadcastable.\n",
    "    if std_test.ndim == 1:\n",
    "        std_test = std_test.reshape(-1, 1)\n",
    "\n",
    "    uq_metrics = evaluate_uq_metrics_from_interval(\n",
    "        y_true=test_dc.y,\n",
    "        mean=mu_test,\n",
    "        lower=lower,\n",
    "        upper=upper,\n",
    "        alpha=alpha,\n",
    "        test_error=test_mse,\n",
    "        weights=test_dc.w,\n",
    "        use_weights=use_weights\n",
    "    )\n",
    "\n",
    "    print(f\"\\n[EVIDENTIAL REGRESSION] Test MSE: {test_mse:.6f}\")\n",
    "    print(f\"[EVIDENTIAL REGRESSION] UQ Metrics: {uq_metrics}\")\n",
    "else:\n",
    "    dc_model = dc.models.TorchModel(\n",
    "        model=model,\n",
    "        loss=loss,\n",
    "        # The output types match the return structure of DenseNormalGamma:\n",
    "        output_types=['prediction', 'loss', 'var1', 'var2'],\n",
    "        batch_size=128,\n",
    "        learning_rate=1e-4,\n",
    "        # wandb=True,  # Set to True\n",
    "        # model_dir='deep-evidential-regression-run-{}'.format(run_id),\n",
    "        log_frequency=40,\n",
    "        mode='classification'\n",
    "    )\n",
    "\n",
    "    # --- 2. Train ---\n",
    "    print(f\"Training Deep Evidential Classification\")\n",
    "    dc_model.fit(train_dc, nb_epoch=10, callbacks=[gradientClip])\n",
    "    device = next(dc_model.model.parameters()).device\n",
    "\n",
    "    # Convert numpy data to PyTorch tensors (DeepChem .X are typically numpy arrays)\n",
    "    valid_X_tensor = torch.from_numpy(valid_dc.X).float().to(device)\n",
    "    test_X_tensor = torch.from_numpy(test_dc.X).float().to(device)\n",
    "\n",
    "    # Get predictions for the validation set\n",
    "    with torch.no_grad():\n",
    "        mu_valid, params_valid, aleatoric_valid, epistemic_valid = dc_model.model(valid_X_tensor)\n",
    "\n",
    "    # Get predictions for the test set\n",
    "    with torch.no_grad():\n",
    "        mu_test, params_test, aleatoric_test, epistemic_test = dc_model.model(test_X_tensor)\n",
    "\n",
    "\n",
    "    # The total predictive variance Var[y] is the sum of aleatoric and epistemic variance.\n",
    "    # Var[y] = E[sigma^2] + Var[mu]\n",
    "    total_var_test = aleatoric_test.cpu().numpy() + epistemic_test.cpu().numpy()\n",
    "    std_test = np.sqrt(total_var_test)\n",
    "\n",
    "    # --- 4. Calculate MSE (using the deterministic mean prediction, gamma) ---\n",
    "    # Ensure prediction shape matches for MSE calculation\n",
    "    mu_test = mu_test[:,1].cpu().numpy()\n",
    "    if mu_test.ndim == 1:\n",
    "        mu_test = mu_test.reshape(-1, 1)\n",
    "\n",
    "    cutoff_error_df = calculate_cutoff_classification_data(mu_test, test_dc.y, test_dc.w, use_weights=use_weights)\n",
    "\n",
    "    test_auc = roc_auc_score(test_dc.y, mu_test, sample_weight=test_dc.w)\n",
    "\n",
    "    # test_mse = mse_from_mean_prediction(mu_test, test_dc)\n",
    "\n",
    "    uq_metrics = evaluate_uq_metrics_classification(\n",
    "        y_true=test_dc.y,\n",
    "        probs=mu_test,\n",
    "        auc=test_auc,\n",
    "        uncertainty=total_var_test,\n",
    "        weights=test_dc.w,\n",
    "        use_weights=use_weights,\n",
    "        n_bins=20\n",
    "    )\n",
    "\n",
    "    print(f\"\\n[EVIDENTIAL CLASSIFIACTION] Test MSE: {test_auc:.6f}\")\n",
    "    print(f\"[EVIDENTIAL CLASSIFIACTION] UQ Metrics: {uq_metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de8c327e-544d-495f-b5a1-bcb484a08ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AUC': [0.688715183297878, 0.8660760428202289, 0.7989885546978971],\n",
       " 'NLL': None,\n",
       " 'Brier': None,\n",
       " 'ECE': None,\n",
       " 'Avg_Entropy': None,\n",
       " 'Spearman_Err_Unc': None}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uq_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0588e62a-0b16-4615-acd1-04127c04b035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1)\n",
      "(5, 1)\n"
     ]
    }
   ],
   "source": [
    "tasks, datasets, transformers = load_delaney()\n",
    "train_dc, valid_dc, test_dc = datasets\n",
    "\n",
    "print(train_dc.y[:3].shape)\n",
    "\n",
    "print(valid_dc.w[:5].shape)\n",
    "\n",
    "# print(train_dc.w[:10])\n",
    "\n",
    "# print(valid_dc.w[:10])\n",
    "\n",
    "# print(test_dc.w[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff2f59d0-cd32-4c0c-be64-759478b34177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DiskDataset X.shape: (np.int64(5470), np.int64(23), np.int64(23)), y.shape: (np.int64(5470), np.int64(1)), w.shape: (np.int64(5470), np.int64(1)), task_names: ['u0_atom']>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc3f93af-d980-44d1-b152-ce73dc7db2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MSE: 0.6796141123379799\n",
      "Test MSE: 0.7320434542583424\n"
     ]
    }
   ],
   "source": [
    "# 2. Define your PyTorch model\n",
    "class MyTorchRegressor(nn.Module):\n",
    "    def __init__(self, n_features, n_tasks):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_tasks)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, n_features)\n",
    "        return self.net(x)  # (batch_size, n_tasks)\n",
    "\n",
    "\n",
    "torch_model = MyTorchRegressor(n_features, n_tasks)\n",
    "\n",
    "# 3. DeepChem loss (NOT torch.nn.MSELoss)\n",
    "loss = dc.models.losses.L2Loss()\n",
    "\n",
    "# 4. Wrap in TorchModel\n",
    "dc_model = dc.models.TorchModel(\n",
    "    model=torch_model,\n",
    "    loss=loss,\n",
    "    output_types=['prediction'],   # normal regression output\n",
    "    batch_size=64,\n",
    "    learning_rate=1e-3,            # DeepChem will create its own Adam optimizer\n",
    "    mode='regression'\n",
    ")\n",
    "\n",
    "# 5. Train\n",
    "dc_model.fit(train_dc, nb_epoch=10)\n",
    "\n",
    "# 6. Evaluate\n",
    "metric = dc.metrics.Metric(dc.metrics.mean_squared_error)\n",
    "valid_scores = dc_model.evaluate(valid_dc, [metric])\n",
    "test_scores = dc_model.evaluate(test_dc, [metric])\n",
    "\n",
    "print(\"Validation MSE:\", valid_scores[metric.name])\n",
    "print(\"Test MSE:\", test_scores[metric.name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "861934ca-3e7d-4e82-9447-b5e0fcb2ef48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "\n",
    "        input_dim = train_x.shape[-1]  # number of features\n",
    "\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(ard_num_dims=input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47c1ddb2-e2a0-48f4-b432-409990a7dce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPyTorchRegressor(nn.Module):\n",
    "    def __init__(self, train_x, train_y):\n",
    "        super().__init__()\n",
    "\n",
    "        # Register training data (for ExactGP)\n",
    "        self.register_buffer(\"train_x\", train_x)\n",
    "        self.register_buffer(\"train_y\", train_y)\n",
    "\n",
    "        # Likelihood + GP model\n",
    "        self.likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "        self.gp_model = ExactGPModel(self.train_x, self.train_y, self.likelihood)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Behaves like a normal PyTorch regressor:\n",
    "        input:  x  (batch_size, input_dim)\n",
    "        output: mean prediction (batch_size, 1)\n",
    "        \"\"\"\n",
    "        self.gp_model.eval()\n",
    "        self.likelihood.eval()\n",
    "        with torch.no_grad():\n",
    "            pred_dist = self.likelihood(self.gp_model(x))\n",
    "            mean = pred_dist.mean  # (batch_size,)\n",
    "            return mean.unsqueeze(-1)  # (batch_size, 1)\n",
    "\n",
    "    def predict_interval(self, x, alpha: float = 0.05):\n",
    "        \"\"\"\n",
    "        Returns mean, lower, upper for a (1 - alpha) confidence interval.\n",
    "        Default alpha=0.05 -> 95% interval.\n",
    "        \"\"\"\n",
    "        self.gp_model.eval()\n",
    "        self.likelihood.eval()\n",
    "        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "            pred_dist = self.likelihood(self.gp_model(x))\n",
    "            mean = pred_dist.mean          # (batch_size,)\n",
    "            std = pred_dist.stddev         # (batch_size,)\n",
    "\n",
    "            # z-score for (1 - alpha) CI, e.g. 1.96 for 95%\n",
    "            z = 1.96 if alpha == 0.05 else torch.distributions.Normal(0, 1).icdf(\n",
    "                torch.tensor(1 - alpha / 2, device=x.device)\n",
    "            )\n",
    "\n",
    "            lower = mean - z * std\n",
    "            upper = mean + z * std\n",
    "            return mean, lower, upper      # each is (batch_size,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "726760dd-7e04-4268-ab04-836a554f62e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: GPyTorchRegressor,\n",
    "        train_dataset: dc.data.NumpyDataset,\n",
    "        lr: float = 0.1,\n",
    "        num_iters: int = 100,\n",
    "        device: str = \"cpu\",\n",
    "        log_interval: int = 10,\n",
    "    ):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.num_iters = num_iters\n",
    "        self.log_interval = log_interval\n",
    "\n",
    "        # Convenience handles\n",
    "        self.likelihood = self.model.likelihood\n",
    "        self.gp = self.model.gp_model\n",
    "\n",
    "        # Convert DeepChem train dataset to torch tensors\n",
    "        self.train_x, self.train_y = self._dc_to_torch(train_dataset)\n",
    "\n",
    "        # ExactMarginalLogLikelihood loss\n",
    "        self.mll = gpytorch.mlls.ExactMarginalLogLikelihood(self.likelihood, self.gp)\n",
    "\n",
    "        # Optimizer (only on GP parameters)\n",
    "        self.optimizer = torch.optim.Adam(self.gp.parameters(), lr=lr)\n",
    "\n",
    "    def _dc_to_torch(self, dataset: dc.data.NumpyDataset):\n",
    "        \"\"\"Convert DeepChem NumpyDataset to torch tensors on the correct device.\"\"\"\n",
    "        X = torch.from_numpy(dataset.X).float().to(self.device)\n",
    "        # dataset.y shape: (N, n_tasks). For Delaney n_tasks=1 so squeeze.\n",
    "        y = torch.from_numpy(dataset.y).float().to(self.device)\n",
    "        if y.ndim == 2 and y.shape[1] == 1:\n",
    "            y = y.squeeze(-1)\n",
    "        return X, y\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Standard Exact GP training loop.\"\"\"\n",
    "        self.model.train()\n",
    "        self.likelihood.train()\n",
    "\n",
    "        for i in range(1, self.num_iters + 1):\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.gp(self.train_x)        # MultivariateNormal\n",
    "            loss = -self.mll(output, self.train_y)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if i % self.log_interval == 0 or i == 1 or i == self.num_iters:\n",
    "                print(f\"[Iter {i:03d}] Loss: {loss.item():.4f}\")\n",
    "\n",
    "    def _predict_mean_tensor(self, dataset):\n",
    "        \"\"\"Internal helper: get mean predictions as tensor (N,) on a dataset.\"\"\"\n",
    "        self.model.eval()\n",
    "        self.likelihood.eval()\n",
    "\n",
    "        X, _ = self._dc_to_torch(dataset)\n",
    "        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "            mean = self.model(X).squeeze(-1)  # (N,)\n",
    "        return mean\n",
    "\n",
    "    def evaluate_mse(self, dataset: dc.data.NumpyDataset) -> float:\n",
    "        \"\"\"Compute mean squared error on a DeepChem dataset.\"\"\"\n",
    "        self.model.eval()\n",
    "        self.likelihood.eval()\n",
    "\n",
    "        X, y_true = self._dc_to_torch(dataset)\n",
    "        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "            mean = self.model(X).squeeze(-1)  # (N,)\n",
    "\n",
    "        mse = torch.mean((mean - y_true) ** 2).item()\n",
    "        return mse\n",
    "\n",
    "    def predict_mean(self, dataset: dc.data.NumpyDataset) -> np.ndarray:\n",
    "        \"\"\"Return mean predictions for a dataset as a NumPy array (N,).\"\"\"\n",
    "        mean = self._predict_mean_tensor(dataset)\n",
    "        return mean.cpu().numpy()\n",
    "\n",
    "    def predict_interval(\n",
    "        self,\n",
    "        dataset: dc.data.NumpyDataset,\n",
    "        alpha: float = 0.05,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Return (mean, lower, upper) as NumPy arrays, each shape (N,).\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        self.likelihood.eval()\n",
    "\n",
    "        X, _ = self._dc_to_torch(dataset)\n",
    "        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "            mean, lower, upper = self.model.predict_interval(X, alpha=alpha)\n",
    "\n",
    "        return (\n",
    "            mean.cpu().numpy(),\n",
    "            lower.cpu().numpy(),\n",
    "            upper.cpu().numpy(),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75f62c8e-d7d0-4aec-9693-4a4a37894546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter 001] Loss: 1.4197\n",
      "[Iter 010] Loss: 1.3223\n",
      "[Iter 020] Loss: 1.0988\n",
      "[Iter 030] Loss: 0.9454\n",
      "[Iter 040] Loss: 0.8729\n",
      "[Iter 050] Loss: 0.8296\n",
      "[Iter 060] Loss: 0.8158\n",
      "[Iter 070] Loss: 0.7857\n",
      "[Iter 080] Loss: 0.7745\n",
      "[Iter 090] Loss: 0.7625\n",
      "[Iter 100] Loss: 0.7648\n",
      "Validation MSE: 0.8327946066856384\n",
      "Test MSE: 0.840823769569397\n"
     ]
    }
   ],
   "source": [
    "# Convert DeepChem train data to torch tensors for model construction\n",
    "X_train_torch = torch.from_numpy(train_dc.X).float()\n",
    "y_train_torch = torch.from_numpy(train_dc.y[:, 0]).float()  # Delaney: 1 task\n",
    "\n",
    "# Build model\n",
    "gp_model_wrapper = GPyTorchRegressor(X_train_torch, y_train_torch)\n",
    "\n",
    "# Build trainer\n",
    "trainer = GPTrainer(\n",
    "    model=gp_model_wrapper,\n",
    "    train_dataset=train_dc,\n",
    "    lr=0.1,\n",
    "    num_iters=100,\n",
    "    device=\"cpu\",\n",
    "    log_interval=10,\n",
    ")\n",
    "\n",
    "# Train GP\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate\n",
    "valid_mse = trainer.evaluate_mse(valid_dc)\n",
    "test_mse = trainer.evaluate_mse(test_dc)\n",
    "print(\"Validation MSE:\", valid_mse)\n",
    "print(\"Test MSE:\", test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f1d4b2e-318d-49a2-9fd1-fd8534298d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 test mean: [-0.9030192   0.0697577  -0.27833986 -1.537761   -0.6202069 ]\n",
      "First 5 lower: [-2.07172   -1.2944871 -1.708732  -2.8169284 -1.8074324]\n",
      "First 5 upper: [ 0.2656815   1.4340025   1.1520523  -0.25859356  0.5670187 ]\n",
      "First 5 gt: [[-1.60114461]\n",
      " [ 0.20848251]\n",
      " [-0.01602738]\n",
      " [-2.82191713]\n",
      " [-0.52891635]]\n"
     ]
    }
   ],
   "source": [
    "# Get uncertainty intervals on test set\n",
    "mean_test, lower_test, upper_test = trainer.predict_interval(test_dc, alpha=0.05)\n",
    "print(\"First 5 test mean:\", mean_test[:5])\n",
    "print(\"First 5 lower:\", lower_test[:5])\n",
    "print(\"First 5 upper:\", upper_test[:5])\n",
    "print(\"First 5 gt:\", test_dc.y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "65388f6f-2ba4-42b5-ab18-837a75b8aab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPyTorchMultitaskRegressor(nn.Module):\n",
    "    \"\"\"\n",
    "    - forward(x) -> mean predictions, shape (N, T)\n",
    "    - predict_interval(x, alpha) -> (mean, lower, upper), each (N, T)\n",
    "    \"\"\"\n",
    "    def __init__(self, train_x, train_y):\n",
    "        super().__init__()\n",
    "\n",
    "        # store training data\n",
    "        self.register_buffer(\"train_x\", train_x)   # (N, D)\n",
    "        self.register_buffer(\"train_y\", train_y)   # (N, T)\n",
    "        self.num_tasks = train_y.shape[-1]\n",
    "\n",
    "        # multitask likelihood\n",
    "        self.likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(\n",
    "            num_tasks=self.num_tasks\n",
    "        )\n",
    "\n",
    "        # multitask GP model\n",
    "        self.gp_model = MultitaskExactGPModel(self.train_x, self.train_y, self.likelihood)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Returns mean prediction (N, T) so it still “looks like”\n",
    "        a normal multitask regressor.\n",
    "        \"\"\"\n",
    "        self.gp_model.eval()\n",
    "        self.likelihood.eval()\n",
    "        with torch.no_grad():\n",
    "            dist = self.likelihood(self.gp_model(x))   # MultitaskMultivariateNormal\n",
    "            mean = dist.mean                           # (N, T)\n",
    "            return mean\n",
    "\n",
    "    def predict_interval(self, x, alpha: float = 0.05):\n",
    "        \"\"\"\n",
    "        Returns (mean, lower, upper), each shape (N, T).\n",
    "        \"\"\"\n",
    "        self.gp_model.eval()\n",
    "        self.likelihood.eval()\n",
    "        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "            dist = self.likelihood(self.gp_model(x))\n",
    "            mean = dist.mean          # (N, T)\n",
    "            std = dist.stddev         # (N, T)\n",
    "\n",
    "            if alpha == 0.05:\n",
    "                z = 1.96\n",
    "            else:\n",
    "                z = torch.distributions.Normal(0, 1).icdf(\n",
    "                    torch.tensor(1 - alpha / 2, device=x.device)\n",
    "                )\n",
    "\n",
    "            lower = mean - z * std\n",
    "            upper = mean + z * std\n",
    "            return mean, lower, upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a6302f53-b64f-4266-b2e7-afcf8a6c7770",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MultitaskExactGPModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m y_train_torch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(train_dc\u001b[38;5;241m.\u001b[39my)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Build model\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m gp_model_wrapper \u001b[38;5;241m=\u001b[39m \u001b[43mGPyTorchMultitaskRegressor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_torch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Build trainer\u001b[39;00m\n\u001b[1;32m      9\u001b[0m trainer \u001b[38;5;241m=\u001b[39m GPTrainer(\n\u001b[1;32m     10\u001b[0m     model\u001b[38;5;241m=\u001b[39mgp_model_wrapper,\n\u001b[1;32m     11\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     log_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     16\u001b[0m )\n",
      "Cell \u001b[0;32mIn[33], line 20\u001b[0m, in \u001b[0;36mGPyTorchMultitaskRegressor.__init__\u001b[0;34m(self, train_x, train_y)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlikelihood \u001b[38;5;241m=\u001b[39m gpytorch\u001b[38;5;241m.\u001b[39mlikelihoods\u001b[38;5;241m.\u001b[39mMultitaskGaussianLikelihood(\n\u001b[1;32m     16\u001b[0m     num_tasks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_tasks\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# multitask GP model\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgp_model \u001b[38;5;241m=\u001b[39m \u001b[43mMultitaskExactGPModel\u001b[49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlikelihood)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MultitaskExactGPModel' is not defined"
     ]
    }
   ],
   "source": [
    "# Convert DeepChem train data to torch tensors for model construction\n",
    "X_train_torch = torch.from_numpy(train_dc.X).float()\n",
    "y_train_torch = torch.from_numpy(train_dc.y).float()\n",
    "\n",
    "# Build model\n",
    "gp_model_wrapper = GPyTorchMultitaskRegressor(X_train_torch, y_train_torch)\n",
    "\n",
    "# Build trainer\n",
    "trainer = GPTrainer(\n",
    "    model=gp_model_wrapper,\n",
    "    train_dataset=train_dc,\n",
    "    lr=0.1,\n",
    "    num_iters=100,\n",
    "    device=\"cpu\",\n",
    "    log_interval=10,\n",
    ")\n",
    "\n",
    "# Train GP\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate\n",
    "valid_mse = trainer.evaluate_mse(valid_dc)\n",
    "test_mse = trainer.evaluate_mse(test_dc)\n",
    "print(\"Validation MSE:\", valid_mse)\n",
    "print(\"Test MSE:\", test_mse)\n",
    "\n",
    "# Get uncertainty intervals on test set\n",
    "mean_test, lower_test, upper_test = trainer.predict_interval(test_dc, alpha=0.05)\n",
    "print(\"First 5 test mean:\", mean_test[:5])\n",
    "print(\"First 5 lower:\", lower_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e98a41-98a1-4700-a697-c2b3a4f1258e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
